# Ensemble models for Fiber Probe analysis
This repository contains models and training routines for analysis of the voltage 
signal from a single-fiber optical bubble probe (A2 Photonic sensors, Grenoble, France).
The goal of the model is to assign a trustworthy speed to bubbles measured by the 
fiber probe. The fiber probe is used as a measurement device in an air-water bubble
column, where gas is sparged with superficial gas velocities ranging from 1 to 15 cm/s.
The models aim to improve on the proprietary Fourier-based algorithms previously
used for the same purpose.

The project was initially implemented for the TU Delft Capstone AI Project (TI3150TU).
In that project, a group of bachelor students set up the working principle for data
gathered at a single superficial gas velocity.
They set up an ensemble of two GRU models and one LSTM model. The whole setup worked
for a single superficial gas velocity, but could not be applied to other conditions.

The work was then extended on by [Luuk Wouterse](www.github.com/luukwouterse) for
his bachelor thesis.
He extended the training regime to multiple superficial gas velocities and extended
the ensemble with two CNN models.
After this, the ensemble predicts accurately for all tested superficial gas velocities.

## Installation

The project depend on the following non-standard Python libraries:
These libraries are used for handling, analyzing, and manipulating data:
- `numpy` - Numerical computations and array processing
- `pandas` - Data manipulation and analysis.
- `scipy` - For signal processing tools.
- `matplotlib` - For plotting.
- `scikit-learn` - Tools for data preprocessing, and evaluation
- `torch` - PyTorch library for building and training neural networks.

## Usage
After cloning the repository, you will have the trained models available.
`main.py` is the core of the whole thing, if you run that file, you will use the
trained models to predict bubble sizes from indicated datasets.
These datasets are indicated in the 'user input' section in `main.py`. Follow
the convention of what is currently there to indicate the data folder
(`path_to_data`) and output folder (`path_to_output`).

To indicate what file(s) to use, you can use a list of integers. This will pick
the n-th files from the folder. (TODO: change to something more descriptive)
The plots-flags are used to turn certain plots on and off. Keep all on for 
maximum information output.

The input folder is expected to contain triplets of .bin, .binlog and .evtlog 
files as generated by the A2 Photonic Sensors fiber probe and software. 

The script will first load in the dataset (might take a while if the data is on
a network drive). It will provide some information about the settings in the file
and save the relevant information to a separate `.csv` file in the output folder.
The user is informed about missing labels in the data and the number of valid
bubbles in the original dataset. Bubble speeds will then be predicted with all
models. The results are saved in a `pandas` dataframe, and a summary is printed
to the command line. 

All requested plots will be saved to the subfolder `Images` in the output folder.

## Main functions

### **1. DataLoading**  
The purpose of data loading is to segment bubble exits from the raw input data. This process involves the following steps:  

#### **Data Extraction and Conversion**  
- Raw binary data (`.bin`) is extracted and transformed using channel coefficients (`Coef1`, `Coef2`), which convert binary voltage signals (`Vtrans`) into decimal voltage values (`Vdata`):  
- Due to the large dataset size (1,482,760,632 data points), voltage values are converted to `float32` format for optimal memory usage.

#### **Segmentation Algorithm**  
- **Downsampling**: The dataset is downsampled by a factor of 5 to reduce computational time.  
- **Smoothing**: A moving average filter (window size = 100) is applied to smooth voltage values, highlighting long-term trends.  
- **Gradient Analysis**: The smoothed signal's gradient is computed to detect significant voltage rises and drops.  
- **Peak Detection**: Peaks corresponding to bubble exits are identified by sudden drops in the signal gradient. These peaks are mapped back to their original data points.  
- **Region of Interest (ROI)**: For each detected bubble peak (`tE`), the ROI is defined as 1,000 points before (`tE0`) and 3,000 points after (`tE1`). These values, along with `Vdata`, are plotted.


#### **Dataset Composition**  
- The final dataset includes:
- Segmented voltage values.
- Exit velocities (if available).
- Acquisition frequency and flow rate for each bubble.
- Validation: The pipeline detects 3,437 bubbles across datasets with varying flow rates, compared to 3,239 events detected by the current bubble analyzer.

---

### **2. Preprocessing**  
Three neural network models (GRU 1, GRU 2, and LSTM) rely on preprocessing pipelines.  

#### **Steps in Preprocessing**  
1. **Frame Waves**:  
 - Voltage values are cropped and zoomed for each bubble.  
 - Two models (GRU 1 and LSTM) use:
   - Signal length = `150`
   - Jump = `0` (focusing on signal ends to reduce noise).  
 - GRU 2 uses:
   - Signal length = `250`
   - Jump = `500`.
2. **Scaling**:  
 - Data is normalized using `sklearn`’s `StandardScaler`, with training data and targets scaled to a mean of 0 and standard deviation of 1.  
 - Scaling parameters:  
   - `feature_scaler_1`: For models with `length=150, jump=0`.  
   - `feature_scaler_2`: For the model with `length=250, jump=500`.  
   - `target_scaler_1`: Common across all models.

---

### **3. Neural Network Models**  
Three models were trained on the segmented and preprocessed data, using a 50-25-25 train-validation-test split for optimization and evaluation.  

#### **GRU 1**  
- **Input**: Sequential data `[batch_size, number of timesteps, number of features]`.  
- **Architecture**: GRU layer → Fully connected layer.  
- **Training**: 1,500 epochs.  
- **Best Hyperparameters**:
- Learning rate: `0.01`  
- Hidden size: `20`  
- Norm: `2`  
- Layers: `2`

#### **GRU 2**  
- **Input**: Similar to GRU 1 but processes different input data.  
- **Architecture**: GRU layer → Fully connected layer.  
- **Training**: 700 epochs.  
- **Best Hyperparameters**:
- Learning rate: `0.02`  
- Hidden size: `20`  
- Norm: `3`  
- Layers: `2`

#### **LSTM**  
- **Input**: Similar to GRU models.  
- **Architecture**: LSTM layer → Fully connected layer.  
- **Training**: 1,500 epochs.  
- **Best Hyperparameters**:
- Learning rate: `0.008`  
- Hidden size: `30`  
- Norm: `3`  
- Layers: `2`

## Used neural networks
To analyze the data we use two GRU models both trained on different preprocessed data. This resulted in a GRU1 and GRU2 model. 
The GRU1 model is trained on sequential data with a length of 150 and 1500 epochs and resulted in a learning rate of 0.01, a hidden size of 20 a norm of 2  and a number of 2 layers.
The GRU2 model is trained on sequential data with a length of 250 and 1500 epochs and resulted in a learning rate of 0.02, a hidden size of 20 a norm of 3  and a number of 2 layers.
The LSTM model was also trained on the sequential data with a lengt of 150 and 1500 epochs and resulted in  a learning rate of 0.008, a hidden size of 30 a norm of 3 and a number of 2 layers.


## Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

## License


# Acknowledgements
## Proof-of-concept
The Capstone AI team set up the initial proof-of-concept of the idea. Many thanks
for proving that the idea was feasible to:
- Silke van Welsen
- Stijn Cuppers
- Bram Benning
- Miró Bruinsma
- Jan-Paul Peters
- Max Hagemann

## Expansion
The project was then built upon by Luuk Wouterse for his Bachelor Thesis project.
This work showed that the models could be trained on an applied to multiple gas
flow rates, greatly expanding the usability of the work.
- Luuk Wouterse
