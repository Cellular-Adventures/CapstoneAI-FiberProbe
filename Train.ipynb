{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d5a22b9-820b-49b6-b84a-2e294d3204e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Voeg het pad toe aan sys.path\n",
    "module_path = os.path.abspath(r'C:\\Users\\luukwouterse\\Downloads/CapstoneAI-FiberProbe-main/CapstoneAI-FiberProbe-main')  # of een specifiek pad zoals: '/mnt/data'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29490623-fb77-47d5-b886-6db678b497da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from advanced_dataloading import process_folder\n",
    "from advanced_preprocessing import frame_waves, valid_velo_data, save_second_scaler, random_noise, \n",
    "duplicate_and_augment_data, flatten_data_distribution, bin_data, calculate_duplication_factors\n",
    "from models import load_scalers, load_models, LSTMModel, GRUModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36de2bee-c984-4bf6-833e-8e51e751f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start user input ###\n",
    "path_to_data = r'U:\\Bubble Column\\Data\\2412_Capstone AI\\Capstone data\\Data'\n",
    "path_to_output = r'H:\\My Documents\\Capstone results'\n",
    "### End user input ###\n",
    "\n",
    "# Libary imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function imports\n",
    "from advanced_dataloading import process_folder\n",
    "from advanced_preprocessing import frame_waves, valid_velo_data\n",
    "from models import load_scalers, load_models, LSTMModel, GRUModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc456ef-a2ed-4339-bc8f-c10903c7db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binlog data extracted\n",
      "{'channelCoef1': 0.0, 'channelCoef2': 6.103701895199438e-05, 'acquisitionFrequency': 20833333.333333332, 'flowRate': 60, 'bin_file': '2024-11-12T145426.bin'}\n"
     ]
    }
   ],
   "source": [
    "df = process_folder(path_to_data, path_to_output, plot=False, labels=True)\n",
    "feature_scaler, target_scaler, X_train, y_train, X_val, y_val, X_test, y_test = save_second_scaler(df)\n",
    "X_train, y_train = random_noise(X_train, y_train, chance, random_seed = 0)\n",
    "hist, bin_indices = bin_data(y_train, bins)\n",
    "factors = calculate_duplication_factors(hist)\n",
    "X_train, y_train = duplicate_and_augment_data(X_train, y_train, bin_indices, factors, noise=0.005)\n",
    "X_train, y_train = flatten_data_distribution(X, y, bins, scaling_factor=0.5, noise=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "103e4207-fcf1-48d0-8218-83ed2dfb4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.01694889 -0.53118426 -0.68171774 ...  0.78896433  0.85746925\n",
      "   1.08142309]\n",
      " [ 0.945617    0.56960979  1.72413608 ...  0.22196841 -0.49849697\n",
      "  -0.36028201]\n",
      " [ 0.89456642 -0.53118426  0.3976509  ... -0.41459003 -0.38295395\n",
      "   0.31563883]\n",
      " ...\n",
      " [ 0.1118083  -0.18066776 -0.23860831 ... -0.00509348 -0.75083158\n",
      "  -0.22483763]\n",
      " [ 1.14130584  1.82393657  2.61035496 ...  0.69840258  0.37404989\n",
      "   0.36122233]\n",
      " [ 0.55140057 -0.4819369   0.94017558 ... -0.59571481 -1.0031649\n",
      "  -0.69759128]]\n",
      "[-0.84383335  0.26776175  0.73419008 -0.54530565 -1.49432351 -0.08341669\n",
      " -1.36141358 -0.50438875 -0.94116624 -0.41358821  0.63323361  0.67041821\n",
      " -0.82010873 -0.35346299  0.35294903 -0.29472791 -0.47172374 -0.06606959\n",
      "  1.68612328 -0.14203647  1.14075343 -1.1505339   0.29859849 -0.77566048\n",
      " -0.28325102 -0.19014785  1.07043604 -1.45096563  2.70326461  0.06383277\n",
      " -0.09672185  3.16178034  0.1481965  -0.53099248 -0.18770687 -0.36622461\n",
      " -0.4972404  -0.56614458 -0.51460727 -1.19114774 -0.74196769  1.10759759\n",
      " -0.08635839 -0.06980188  0.7806379   2.50597668 -0.91699361  0.25570838\n",
      "  0.58867664  0.83456678 -0.15309171  1.5262242   0.50095944  1.07948843\n",
      " -0.2655712  -0.90537177  0.28917057 -1.462739    0.19559963 -0.58565925\n",
      "  0.51458743 -0.53987686 -0.19293142 -0.10974371 -1.31571354  0.46947046\n",
      "  0.53933984 -0.12388888 -0.9615934   0.29017529 -1.00385103  0.6639847\n",
      " -0.32160506 -1.16363482 -0.21816807 -1.52764735  2.21417897  0.16626503\n",
      "  0.79071147 -0.51123404 -0.63142847  0.67792563 -0.22604444 -0.28508258\n",
      " -0.8283211   0.20254704 -0.75217303 -0.09210672  2.24320062  0.67846258\n",
      " -0.11695795  1.24773494 -0.54145806  0.31746421  2.4259942   0.06566763\n",
      " -0.66559891 -1.09139034 -1.07459995  0.35831853 -0.38206957 -1.05753285\n",
      " -0.46951665  1.54309036 -0.18825041  1.61490329  1.48316938 -0.77007027\n",
      " -0.2152132  -1.04240931 -1.2416935  -1.15700694  0.58622578 -0.35721505\n",
      "  2.43218724  0.80461288  0.62966272 -0.03996987 -1.02117178 -0.76708904\n",
      "  0.46986905  1.12287925  2.11373968 -1.27331755 -0.42275918  0.69942669\n",
      " -0.18558213 -0.76382781  0.12330245  1.21116964  0.31718421 -1.14693666\n",
      " -0.18148088  1.80085927  1.07523236 -0.37681537  0.64896657  0.55864368\n",
      " -0.51815509  1.21255319 -0.92490621  0.6016985  -0.26804183 -1.36844004\n",
      "  0.72189953  0.11278416  1.43349327  0.9078588  -1.17077987  0.26701727\n",
      "  0.17321244 -0.93537179 -0.85020758  0.82111009 -1.20269051  1.50573445\n",
      " -0.36757192  0.02432907 -0.42024902  0.87189962  0.94382127 -1.13771298\n",
      " -0.40474665 -0.66674199  1.89372197  0.88168002 -0.52207516 -1.61551938\n",
      "  0.35883242  0.06477161 -0.06354954 -0.29540322  0.06665588 -0.07507256\n",
      " -1.76871153 -1.17217001 -0.66116496  0.30320704 -1.36413127 -1.11964445\n",
      " -1.18848275  0.01451573  1.07043604 -0.42864586  0.74890185 -1.23946994\n",
      "  0.77284718 -1.58713679  0.44494206 -1.01741313 -0.29346624  0.29309064\n",
      " -0.84033494 -1.40604631  0.25523731  2.20778828 -0.26813406 -0.8912266\n",
      "  0.71067299 -0.56187534 -0.18079899 -1.30605173 -0.20670765 -0.76424617\n",
      " -0.68274837  0.91385419 -1.55013666  1.25234678 -0.55165023  0.12154666\n",
      " -0.65969905 -0.620449   -0.57489061  0.46517815 -1.39494165 -0.57682758\n",
      " -0.55296131 -0.59384198 -1.08124759  1.32870568 -1.45267201 -0.74512351\n",
      " -0.64096839  2.60602067 -0.829339    1.16333497  0.22066828 -1.6494164\n",
      " -0.06564793  0.9815593   0.12074617 -1.21642391 -0.69732837  0.2028501\n",
      "  0.94981337 -1.18848605  0.27443245 -0.39791124  0.37838003  0.43691746\n",
      " -1.05062826 -0.24466968  0.55818579 -0.83991329  0.67634113 -0.4997835\n",
      " -0.18975584 -0.22643974 -0.12760471 -1.06678617 -0.43130755  0.12836559\n",
      "  0.44995908 -1.39729369  0.53933984 -0.11715889  2.54277257 -0.23564695\n",
      "  0.70313263  0.08534701 -0.64739861 -0.60766432  3.7416204   0.40182465\n",
      " -0.13421611  0.93183378 -0.02512303  1.11227202 -0.12377359 -0.02527457\n",
      "  0.10416331 -0.92764037 -0.44346305  0.10469367  0.33087478 -1.51881897\n",
      " -0.59949807  0.28893339 -1.14560581  0.44082105 -0.86247178  1.43899454\n",
      " -0.73052044  0.81827051 -0.55395944 -0.84988145 -0.45803976 -0.04005881\n",
      " -0.17550855 -1.24082714  1.64379318  0.24031471 -0.58103424 -0.49651568\n",
      " -0.27767728  1.67755844  2.94502385 -0.02933299  2.50811789 -0.47580193\n",
      "  0.38764324 -0.48793107 -0.81813882 -0.59447775  1.20757899  0.60999323\n",
      "  0.04901888  0.52838013 -0.18896195  0.09140499  0.88388382 -0.33971312\n",
      "  1.92024005  2.89037355 -0.04875542 -1.19583205 -0.40748411 -0.13683827\n",
      " -1.28376666  3.57213528  0.80690562 -0.46076074 -0.29558769 -1.52655698\n",
      "  1.08934788 -0.90898218  0.12010381  0.18955812 -0.46952324 -0.39141514\n",
      " -1.22224471  1.86348147 -0.20364407 -0.20245158 -0.16306976  0.15297305\n",
      " -1.38887708 -1.41816886 -0.35359805  0.33817796 -0.29114056  0.87442296\n",
      "  0.63931465  0.34484536 -1.05661377 -0.94662139 -1.60061654 -0.80241244\n",
      "  0.43903231  1.110816   -0.17011599  1.20174831 -0.18075287 -0.44916855\n",
      " -0.18124041  1.40733755  1.15205903  0.49824175 -1.32239082 -1.6794263\n",
      "  3.60408874  0.03815141  2.3005192  -0.14751139  1.04900746 -0.44021499\n",
      " -0.39847455 -0.61760942 -1.46411267 -0.54771699  0.50288983  0.26969543\n",
      "  1.88495947 -0.56453044 -1.11963127  1.02009122 -0.17001058  0.21372087\n",
      " -0.92838815 -0.40784317  0.25325422  0.29721823 -0.98943903 -1.02611963\n",
      "  1.38085241  0.35434905  0.58446999 -0.05302137 -1.25036706  0.97591968\n",
      "  0.14609482 -0.78992094 -0.86164823 -0.64217076 -0.94415735 -1.53523712\n",
      " -1.35191319 -0.57786524  2.52999119 -0.18276232  0.44803529 -1.0721425\n",
      " -0.95644461 -0.96115857 -1.32035173 -0.37702949 -0.46840651 -0.24077268\n",
      "  0.65147014 -0.53809801 -0.37345531  0.82363342 -0.81548701  2.13478944\n",
      " -0.50103199 -0.04370216 -0.89594715 -0.6452607   0.79819583 -0.11165433\n",
      "  0.12986115  1.87118984 -1.2344628   0.02738606 -0.20359136  0.03585208\n",
      "  0.10958552  0.26356168  0.87505215 -0.3063201  -0.41047851 -0.26355187\n",
      "  2.68873731  0.08655926  2.65325909 -0.17433912  0.03654056 -1.3518506\n",
      " -0.05811416  0.07676898 -0.16777384 -0.30180709  1.37228757 -1.13941607\n",
      " -0.52624228 -0.28577106 -0.01307954  0.84855383 -0.19265471  1.10939292\n",
      "  2.23341693  2.63102343  0.26058375  0.39314121  0.1590508  -0.95948842\n",
      " -1.49626707 -0.49691098 -0.85266832  1.12287925]\n",
      "[0.630479  0.591366  0.744747  0.718764  0.873908  0.20268   0.402324\n",
      " 0.526221  0.440362  0.592111  0.515928  0.404871  0.306578  0.980458\n",
      " 0.377656  0.638981  0.244986  0.302831  0.314487  0.531035  0.986599\n",
      " 0.202224  1.09299   0.6524    0.354187  0.510731  0.343882  0.517347\n",
      " 0.514272  1.40019   1.03578   0.794157  0.473752  0.391282  0.678956\n",
      " 1.60559   0.373508  0.671531  0.142325  0.0528072 1.32602   0.403618\n",
      " 0.389742  0.720752  0.560786  0.682884  0.787318  0.390649  1.17278\n",
      " 0.270767  0.23673   0.276794  0.410317  0.542424  0.632129  0.540316\n",
      " 0.449475  1.99503   0.554225  0.611476  0.634833  0.395322  1.44081\n",
      " 0.410302  0.841153  0.595284  0.356704  0.839525  0.403341  0.748043\n",
      " 0.310666  0.626243  1.34029   0.212471  0.360465  0.366095  0.379534\n",
      " 0.87563   0.397328  0.555655  0.364988  0.243624  0.419569  0.668528\n",
      " 0.337886  0.833437  0.499899  0.459331  0.5006    0.614911  0.512048\n",
      " 0.561786  0.302925  0.569481  1.38881   0.460507  0.449656  0.75944\n",
      " 0.583366  1.53158   0.357404  2.66446   0.46316   0.334009  0.628554\n",
      " 0.743948  0.717129  0.457775  0.805478  0.41409   0.507928  0.749959\n",
      " 0.550517  0.521638  0.705569  0.372596  0.348443  0.554001  1.35944\n",
      " 0.814757  0.289034  0.494119  0.553499  0.306532  0.974318  0.813734\n",
      " 0.0807682 0.532889  0.332287  0.612455  0.310989  0.62277   0.600169\n",
      " 0.812961  0.753183  0.649241  1.29458   0.433028  0.750773  0.840693\n",
      " 1.29868   0.490204  0.488025  0.654445  0.319442  0.685465  0.345583\n",
      " 0.444511  0.716299  0.355111  0.410042  0.228222  0.798152  0.629148\n",
      " 0.880094  0.42648   0.448195  1.29302   0.58054   1.36696   0.27718\n",
      " 0.715631  0.738317  0.808497  0.101428  0.941304  0.436407  0.327333\n",
      " 0.615562  0.688106  0.484542  0.55618   0.581416  0.523049  0.603715\n",
      " 0.1578    0.401113  0.724813  0.576307  0.758124  0.566555  0.773809\n",
      " 0.58842   0.537202  0.65704   0.54571   0.769851  0.282351  0.535328\n",
      " 0.640044  0.330286  0.293978  0.69794   0.748467  0.937184  0.273155\n",
      " 0.477421  0.485806  0.468509  0.28709   0.211213  1.19039   0.535602\n",
      " 0.605422  0.418005  0.96845   0.688668  2.32975   0.518102  0.586533\n",
      " 1.00312   0.883373  0.319952  0.471969  0.385105  0.310738  0.506253\n",
      " 0.46053   0.279908  0.67004   0.456038  0.672219  1.82491   0.515249\n",
      " 0.34273   0.443628  0.630816 ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98bf53b-a739-4023-85bd-2177d61f3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train[..., np.newaxis], dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val[..., np.newaxis], dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Model config from your best setup\n",
    "model = GRUModel(input_size=1, hidden_size=20, num_layers=2)\n",
    "model.to('cpu')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1500\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    loss = criterion(y_pred_train, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_tensor)\n",
    "        val_loss = criterion(y_pred_val, y_val_tensor)\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {loss.item():.4f} - Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47194d5f-bbeb-461f-b3a9-46d28baa0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.030 cm/s\n",
      "MAE: 0.138 cm/s\n",
      "R² Score: -1.657\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_scaled = model(X_val_tensor).detach().cpu().numpy().flatten()\n",
    "\n",
    "# Inverse-transform\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_true = target_scaler.inverse_transform(y_val_tensor.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metrics\n",
    "rmse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f} cm/s\")\n",
    "print(f\"MAE: {mae:.3f} cm/s\")\n",
    "print(f\"R² Score: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f99a3c-d116-44b2-b0ce-ecffdaf0645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction range: 0.68257177 0.6931912\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction range:\", y_pred.min(), y_pred.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83588c-14fc-410b-a51c-370830cf658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru2_model = GRUModel(input_size=1, hidden_size=20, num_layers=2)\n",
    "gru2_model.to('cpu')\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(gru2_model.parameters(), lr=0.02)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "epochs = 700\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gru2_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred_train = gru2_model(X_train_tensor)\n",
    "    loss = criterion(pred_train, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    gru2_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_val = gru2_model(X_val_tensor)\n",
    "        val_loss = criterion(pred_val, y_val_tensor)\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {loss.item():.4f} - Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title(\"GRU 2 Training Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d56cf1-2614-4bf4-9774-8a9698ee49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_size=1, hidden_size=30, num_layers=2)\n",
    "lstm_model.to('cpu')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.008)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1500\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lstm_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_train = lstm_model(X_train_tensor)\n",
    "    loss = criterion(y_pred_train, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = lstm_model(X_val_tensor)\n",
    "        val_loss = criterion(y_pred_val, y_val_tensor)\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {loss.item():.4f} - Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title(\"LSTM Training Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
